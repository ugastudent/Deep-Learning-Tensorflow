{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BRG_user\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 1s 841us/step - loss: 0.7008 - acc: 0.5210\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 0.6918 - acc: 0.5460\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 0.6885 - acc: 0.5440\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 0.6839 - acc: 0.5520\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 89us/step - loss: 0.6801 - acc: 0.5590\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 0.6821 - acc: 0.5650\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 110us/step - loss: 0.6769 - acc: 0.5740\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.6738 - acc: 0.5960\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 0.6725 - acc: 0.5760\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.6691 - acc: 0.5920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2720bf211d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(2, size=(1000, 1))\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 2.4115 - acc: 0.0830\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 2.3585 - acc: 0.0940\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 2.3451 - acc: 0.1050\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 2.3331 - acc: 0.0970\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 2.3269 - acc: 0.0980\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 2.3236 - acc: 0.0930\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 2.3126 - acc: 0.1090\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 2.3099 - acc: 0.1070\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 2.3048 - acc: 0.1050\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 2.3128 - acc: 0.0970\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 2.3079 - acc: 0.1030\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 2.2954 - acc: 0.1060\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 2.3090 - acc: 0.1200\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 2.3080 - acc: 0.1070\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 2.3054 - acc: 0.1000\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 2.2913 - acc: 0.1170\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 2.3035 - acc: 0.1010\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 2.3048 - acc: 0.0940\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 2.2998 - acc: 0.1100\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 2.2938 - acc: 0.1200\n",
      "100/100 [==============================] - 0s 737us/step\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "x_train = np.random.random((1000, 20))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\n",
    "x_test = np.random.random((100, 20))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "\n",
    "model = Sequential()\n",
    "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "# in the first layer, you must specify the expected input data shape:\n",
    "# here, 20-dimensional vectors.\n",
    "model.add(Dense(64, activation='relu', input_dim=20))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128)\n",
    "score = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.7180 - acc: 0.5120\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.7056 - acc: 0.5050\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.6993 - acc: 0.5170\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 0.7039 - acc: 0.4970\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.6989 - acc: 0.5120\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.7018 - acc: 0.5140\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.7024 - acc: 0.4960\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.6945 - acc: 0.5020\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.6983 - acc: 0.5050\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.6987 - acc: 0.5090\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.6906 - acc: 0.5420\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.6981 - acc: 0.5020\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.6890 - acc: 0.5290\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.6909 - acc: 0.5330\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.6933 - acc: 0.5050\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.6915 - acc: 0.5250\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.6887 - acc: 0.5420\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 0.6927 - acc: 0.5260\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.6878 - acc: 0.5580\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.6929 - acc: 0.5320\n",
      "100/100 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# Generate dummy data\n",
    "x_train = np.random.random((1000, 20))\n",
    "y_train = np.random.randint(2, size=(1000, 1))\n",
    "x_test = np.random.random((100, 20))\n",
    "y_test = np.random.randint(2, size=(100, 1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=20, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128)\n",
    "score = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 2.3246\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 5s 45ms/step - loss: 2.3157\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 4s 45ms/step - loss: 2.3078\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 4s 44ms/step - loss: 2.3018\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 2.3007\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 5s 46ms/step - loss: 2.2893\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 4s 44ms/step - loss: 2.2928\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 4s 42ms/step - loss: 2.2945\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 5s 46ms/step - loss: 2.2938\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 5s 46ms/step - loss: 2.3071\n",
      "20/20 [==============================] - 0s 21ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Generate dummy data\n",
    "x_train = np.random.random((100, 100, 100, 3))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "x_test = np.random.random((20, 100, 100, 3))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(20, 1)), num_classes=10)\n",
    "\n",
    "model = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=10)\n",
    "score = model.evaluate(x_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 11.5316 - acc: 0.1080 - val_loss: 11.0240 - val_acc: 0.1700\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 350us/step - loss: 11.5297 - acc: 0.1030 - val_loss: 11.0265 - val_acc: 0.1700\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 362us/step - loss: 11.5289 - acc: 0.1100 - val_loss: 11.0259 - val_acc: 0.1600\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 321us/step - loss: 11.5283 - acc: 0.1020 - val_loss: 11.0258 - val_acc: 0.1700\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 320us/step - loss: 11.5278 - acc: 0.1160 - val_loss: 11.0241 - val_acc: 0.1700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x272122a0ef0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "data_dim = 16\n",
    "timesteps = 8\n",
    "num_classes = 10\n",
    "\n",
    "# expected input data shape: (batch_size, timesteps, data_dim)\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True,\n",
    "               input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 32\n",
    "model.add(LSTM(32, return_sequences=True))  # returns a sequence of vectors of dimension 32\n",
    "model.add(LSTM(32))  # return a single vector of dimension 32\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy training data\n",
    "x_train = np.random.random((1000, timesteps, data_dim))\n",
    "y_train = np.random.random((1000, num_classes))\n",
    "\n",
    "# Generate dummy validation data\n",
    "x_val = np.random.random((100, timesteps, data_dim))\n",
    "y_val = np.random.random((100, num_classes))\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64, epochs=5,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 320 samples, validate on 96 samples\n",
      "Epoch 1/5\n",
      "320/320 [==============================] - 7s 22ms/step - loss: 11.7924 - acc: 0.1219 - val_loss: 11.8659 - val_acc: 0.0833\n",
      "Epoch 2/5\n",
      "320/320 [==============================] - 0s 490us/step - loss: 11.7884 - acc: 0.1438 - val_loss: 11.8669 - val_acc: 0.0729\n",
      "Epoch 3/5\n",
      "320/320 [==============================] - 0s 500us/step - loss: 11.7872 - acc: 0.1469 - val_loss: 11.8672 - val_acc: 0.0729\n",
      "Epoch 4/5\n",
      "320/320 [==============================] - 0s 445us/step - loss: 11.7864 - acc: 0.1531 - val_loss: 11.8673 - val_acc: 0.0729\n",
      "Epoch 5/5\n",
      "320/320 [==============================] - 0s 479us/step - loss: 11.7855 - acc: 0.1531 - val_loss: 11.8674 - val_acc: 0.0729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2721a926748>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "data_dim = 16\n",
    "timesteps = 8\n",
    "num_classes = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Expected input batch shape: (batch_size, timesteps, data_dim)\n",
    "# Note that we have to provide the full batch_input_shape since the network is stateful.\n",
    "# the sample of index i in batch k is the follow-up for the sample i in batch k-1.\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True, stateful=True,\n",
    "               batch_input_shape=(batch_size, timesteps, data_dim)))\n",
    "model.add(LSTM(32, return_sequences=True, stateful=True))\n",
    "model.add(LSTM(32, stateful=True))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy training data\n",
    "x_train = np.random.random((batch_size * 10, timesteps, data_dim))\n",
    "y_train = np.random.random((batch_size * 10, num_classes))\n",
    "\n",
    "# Generate dummy validation data\n",
    "x_val = np.random.random((batch_size * 3, timesteps, data_dim))\n",
    "y_val = np.random.random((batch_size * 3, num_classes))\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size, epochs=5, shuffle=False,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 0s 414us/step - loss: 2.3270 - acc: 0.0900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17a1d6c8b70>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "import numpy as np\n",
    "# This returns a tensor\n",
    "inputs = Input(shape=(20,))\n",
    "\n",
    "x_train = np.random.random((1000, 20))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\n",
    "\n",
    "\n",
    "\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "x = Dense(64, activation='relu')(inputs)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# This creates a model that includes\n",
    "# the Input layer and three Dense layers\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train)  # starts training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 20, 512)\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5304 - main_output_loss: 0.3900 - aux_output_loss: 0.7023\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4661 - main_output_loss: 0.3273 - aux_output_loss: 0.6939\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4660 - main_output_loss: 0.3272 - aux_output_loss: 0.6937\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4655 - main_output_loss: 0.3268 - aux_output_loss: 0.6936\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4649 - main_output_loss: 0.3261 - aux_output_loss: 0.6936\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4652 - main_output_loss: 0.3265 - aux_output_loss: 0.6936\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4649 - main_output_loss: 0.3262 - aux_output_loss: 0.6936\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4648 - main_output_loss: 0.3261 - aux_output_loss: 0.6936\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4644 - main_output_loss: 0.3257 - aux_output_loss: 0.6936\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4639 - main_output_loss: 0.3253 - aux_output_loss: 0.6934\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4644 - main_output_loss: 0.3257 - aux_output_loss: 0.6935\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4637 - main_output_loss: 0.3250 - aux_output_loss: 0.6934\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4635 - main_output_loss: 0.3248 - aux_output_loss: 0.6935\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4632 - main_output_loss: 0.3245 - aux_output_loss: 0.6936\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4627 - main_output_loss: 0.3240 - aux_output_loss: 0.6934\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4627 - main_output_loss: 0.3240 - aux_output_loss: 0.6935\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4626 - main_output_loss: 0.3240 - aux_output_loss: 0.6934\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4622 - main_output_loss: 0.3235 - aux_output_loss: 0.6934\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4619 - main_output_loss: 0.3232 - aux_output_loss: 0.6934\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4607 - main_output_loss: 0.3221 - aux_output_loss: 0.6934\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4612 - main_output_loss: 0.3225 - aux_output_loss: 0.6934\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4603 - main_output_loss: 0.3216 - aux_output_loss: 0.6934\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4601 - main_output_loss: 0.3214 - aux_output_loss: 0.6935\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4597 - main_output_loss: 0.3210 - aux_output_loss: 0.6933\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4594 - main_output_loss: 0.3207 - aux_output_loss: 0.6933\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4581 - main_output_loss: 0.3194 - aux_output_loss: 0.6934\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4590 - main_output_loss: 0.3203 - aux_output_loss: 0.6934\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4582 - main_output_loss: 0.3195 - aux_output_loss: 0.6933\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4582 - main_output_loss: 0.3195 - aux_output_loss: 0.6933\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4577 - main_output_loss: 0.3190 - aux_output_loss: 0.6934\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4572 - main_output_loss: 0.3185 - aux_output_loss: 0.6934\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4564 - main_output_loss: 0.3178 - aux_output_loss: 0.6934\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4561 - main_output_loss: 0.3175 - aux_output_loss: 0.6933\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4564 - main_output_loss: 0.3177 - aux_output_loss: 0.6933\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4551 - main_output_loss: 0.3164 - aux_output_loss: 0.6933\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4555 - main_output_loss: 0.3168 - aux_output_loss: 0.6933\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4539 - main_output_loss: 0.3152 - aux_output_loss: 0.6933\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4540 - main_output_loss: 0.3154 - aux_output_loss: 0.6933\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4537 - main_output_loss: 0.3150 - aux_output_loss: 0.6933\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4529 - main_output_loss: 0.3142 - aux_output_loss: 0.6933\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4530 - main_output_loss: 0.3143 - aux_output_loss: 0.6933\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4523 - main_output_loss: 0.3137 - aux_output_loss: 0.6933\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4513 - main_output_loss: 0.3126 - aux_output_loss: 0.6933\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4507 - main_output_loss: 0.3120 - aux_output_loss: 0.6933\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4505 - main_output_loss: 0.3118 - aux_output_loss: 0.6933\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4496 - main_output_loss: 0.3109 - aux_output_loss: 0.6933\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4493 - main_output_loss: 0.3106 - aux_output_loss: 0.6933\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4485 - main_output_loss: 0.3098 - aux_output_loss: 0.6933\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4482 - main_output_loss: 0.3095 - aux_output_loss: 0.6933\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4471 - main_output_loss: 0.3084 - aux_output_loss: 0.6933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17a22eb0a90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "x_train = np.random.random((1000, 20))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\n",
    "# Headline input: meant to receive sequences of 100 integers, between 1 and 10000.\n",
    "# Note that we can name any layer by passing it a \"name\" argument.\n",
    "main_input = Input(shape=(20,), dtype='int32', name='main_input')\n",
    "\n",
    "# This embedding layer will encode the input sequence\n",
    "# into a sequence of dense 512-dimensional vectors.\n",
    "x = Embedding(output_dim=512, input_dim=10000, input_length=100)(main_input)\n",
    "print(np.shape(x))\n",
    "# A LSTM will transform the vector sequence into a single vector,\n",
    "# containing information about the entire sequence\n",
    "lstm_out = LSTM(32)(x)\n",
    "auxiliary_output = Dense(20, activation='sigmoid', name='aux_output')(lstm_out)\n",
    "auxiliary_input = Input(shape=(20,), name='aux_input')\n",
    "x = keras.layers.concatenate([lstm_out, auxiliary_input])\n",
    "\n",
    "# We stack a deep densely-connected network on top\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "\n",
    "# And finally we add the main logistic regression layer\n",
    "main_output = Dense(10, activation='sigmoid', name='main_output')(x)\n",
    "model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output, auxiliary_output])\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy',\n",
    "              loss_weights=[1., 0.2])\n",
    "model.fit([x_train, x_train], [y_train, x_train],\n",
    "          epochs=50, batch_size=32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(4000, output_dim=640, input_length=10))\n",
    "# the model will take as input an integer matrix of size (batch, input_length).\n",
    "# the largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size).\n",
    "# now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
    "\n",
    "input_array = np.random.randint(3000, size=(32, 10))\n",
    "\n",
    "model.compile('rmsprop', 'mse')\n",
    "output_array = model.predict(input_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 10, 640)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(output_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
